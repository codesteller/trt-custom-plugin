{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorrt as trt\n",
    "import uff\n",
    "import graphsurgeon as gs\n",
    "from tf_utils.model import Model\n",
    "from keras.backend import get_session\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import ctypes\n",
    "import pycuda.driver as cuda\n",
    "# This import causes pycuda to automatically manage CUDA context creation and cleanup.\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CDLL './geluPluginv2/build_trt7_demo/libGeluPlugin.so', handle 5066de0 at 0x7f0b741eb390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add plugin compiled library\n",
    "ctypes.CDLL(\"./geluPluginv2/build_trt7_demo/libGeluPlugin.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add paths here\n",
    "final_checkpoint = \"./saved_model/checkpoints/saved_model-0005.h5\"\n",
    "frozen_filename = \"./saved_model/frozen_model/model.pb\"\n",
    "uff_filename = \"./saved_model/frozen_model/model.uff\"\n",
    "if not os.path.exists(\"saved_model/frozen_model\"):\n",
    "        os.makedirs(\"saved_model/frozen_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n",
      "7.0.0.11\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(trt.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "* Create a plugin node and replace the unsupported layer with TensorRT custom layer/plugin\n",
    "* Load a trained model, freeze the graph and convert to UFF file\n",
    "* Build TensorRT Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plugin_node(dynamic_graph):\n",
    "    gelu_node = gs.create_plugin_node(\n",
    "        name=\"GeluActivation\", op=\"CustomGeluPlugin\", typeId=0)\n",
    "    namespace_plugin_map = {\"GeluActivation\": gelu_node}\n",
    "    dynamic_graph.collapse_namespaces(namespace_plugin_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_uff(model, frozen_filename, uff_filename):\n",
    "    # First freeze the graph and remove training nodes.\n",
    "    # output_name is \"dense_2/MatMul\" for verification\n",
    "    output_names = model.output.op.name\n",
    "    sess = get_session()\n",
    "    frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "        sess, sess.graph.as_graph_def(), [output_names])\n",
    "    frozen_graph = tf.graph_util.remove_training_nodes(frozen_graph)\n",
    "    # Save the model\n",
    "    with open(frozen_filename, \"wb\") as fptr:\n",
    "        fptr.write(frozen_graph.SerializeToString())\n",
    "\n",
    "    # Transform graph using graphsurgeon to map unsupported TensorFlow\n",
    "    # operations to appropriate TensorRT custom layer plugins\n",
    "    dynamic_graph = gs.DynamicGraph(frozen_graph)\n",
    "    create_plugin_node(dynamic_graph)\n",
    "\n",
    "    uff_model = uff.from_tensorflow(dynamic_graph, [output_names])\n",
    "    with open(uff_filename, \"wb\") as fptr:\n",
    "        fptr.write(uff_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_engine(model_file, TRT_LOGGER):\n",
    "    # For more information on TRT basics, refer to the introductory samples.\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network() as network, trt.UffParser() as parser:\n",
    "        builder.max_workspace_size = 1 << 16\n",
    "        # Parse the Uff Network\n",
    "        parser.register_input(\"input_1\", (3, 150, 150))\n",
    "        parser.register_output('dense_2/Softmax')\n",
    "        parser.parse(model_file, network)\n",
    "        # Build and return an engine.\n",
    "        return builder.build_cuda_engine(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Model\n",
    "* Convert the Model to UFF file\n",
    "* Convert the UFF to TensorRT Engine File\n",
    "* Save the serialized TensorRT Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model built and compiled successfully\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "cnn_model = Model(input_shape=(150, 150, 3))\n",
    "cnn_model.build_model()\n",
    "cnn_model.model.load_weights(final_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to UFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-78f913d705f0>:7: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 10 variables.\n",
      "INFO:tensorflow:Converted 10 variables to const ops.\n",
      "WARNING:tensorflow:From <ipython-input-6-78f913d705f0>:8: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.remove_training_nodes`\n",
      "NOTE: UFF has been tested with TensorFlow 1.14.0.\n",
      "WARNING: The version of TensorFlow installed on this system is not guaranteed to work with UFF.\n",
      "UFF Version 0.6.5\n",
      "=== Automatically deduced input nodes ===\n",
      "[name: \"input_1\"\n",
      "op: \"Placeholder\"\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shape\"\n",
      "  value {\n",
      "    shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "      dim {\n",
      "        size: 150\n",
      "      }\n",
      "      dim {\n",
      "        size: 150\n",
      "      }\n",
      "      dim {\n",
      "        size: 3\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]\n",
      "=========================================\n",
      "\n",
      "Using output node dense_2/Softmax\n",
      "Converting to UFF graph\n",
      "Warning: No conversion function registered for layer: CustomGeluPlugin yet.\n",
      "Converting GeluActivation as custom op: CustomGeluPlugin\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/uff/converters/tensorflow/converter.py:179: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead.\n",
      "\n",
      "DEBUG: convert reshape to flatten node\n",
      "DEBUG [/usr/local/lib/python3.6/dist-packages/uff/converters/tensorflow/converter.py:96] Marking ['dense_2/Softmax'] as outputs\n",
      "No. nodes: 31\n"
     ]
    }
   ],
   "source": [
    "convert_to_uff(model=cnn_model.model, frozen_filename=frozen_filename, uff_filename=uff_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert UFF to TensorRT Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_filename = \"./saved_model/frozen_model/model.engine\"\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = build_engine(uff_filename, TRT_LOGGER)\n",
    "with open(engine_filename, \"wb\") as f:\n",
    "    f.write(engine.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with TensorRT Engine\n",
    "## Helper functions\n",
    "* To run the inference with streaming input\n",
    "* Allocate memory in the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n",
    "    # Transfer input data to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "    # Run inference.\n",
    "    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n",
    "    # Transfer predictions back from the GPU.\n",
    "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    # Return only the host outputs.\n",
    "    return [out.host for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_buffers(engine):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "    for binding in engine:\n",
    "        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
    "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "        # Allocate host and device buffers\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        # Append the device buffer to device bindings.\n",
    "        bindings.append(int(device_mem))\n",
    "        # Append to the appropriate list.\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        else:\n",
    "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "    return inputs, outputs, bindings, stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = \"./test_data\"\n",
    "TEST_SAMPLES = 10\n",
    "test_images = glob(os.path.join(TEST_DATA_PATH, \"*.jpg\"))[:TEST_SAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved Engine File\n",
    "with open(engine_filename, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    # trt.init_libnvinfer_plugins(TRT_LOGGER)\n",
    "    # Note that we have to provide the plugin factory when deserializing an engine built with an IPlugin or IPluginExt.\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55680126 0.44319874]\n",
      "cat.8.jpg : Prediction -  Cat\n",
      "[0.7626318  0.23736823]\n",
      "cat.7.jpg : Prediction -  Cat\n",
      "[0.67725796 0.32274204]\n",
      "cat.10.jpg : Prediction -  Cat\n",
      "[0.21873236 0.78126764]\n",
      "dog.4.jpg : Prediction -  Dog\n",
      "[0.43486238 0.5651376 ]\n",
      "dog.3.jpg : Prediction -  Dog\n",
      "[0.86916035 0.13083966]\n",
      "cat.5.jpg : Prediction -  Cat\n",
      "[0.5476549 0.4523451]\n",
      "cat.6.jpg : Prediction -  Cat\n",
      "[0.22099385 0.7790062 ]\n",
      "dog.2.jpg : Prediction -  Dog\n",
      "[0.2710958 0.7289041]\n",
      "dog.1.jpg : Prediction -  Dog\n",
      "[0.72174346 0.2782565 ]\n",
      "cat.9.jpg : Prediction -  Cat\n"
     ]
    }
   ],
   "source": [
    "# Do prediction\n",
    "with engine.create_execution_context() as context:\n",
    "    inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "    for impath in test_images:\n",
    "        img = Image.open(impath)\n",
    "        img = img.resize((150, 150))\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = np.expand_dims(img, axis=0) / 255.0\n",
    "        img = img.ravel()\n",
    "        inputs[0].host = img\n",
    "\n",
    "        [output] = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "        print(output)\n",
    "        pred = np.argmax(output)\n",
    "        basename = os.path.basename(impath)\n",
    "        if pred:\n",
    "            print(\"{} : Prediction -  Dog\".format(basename))\n",
    "        else:\n",
    "            print(\"{} : Prediction -  Cat\".format(basename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
