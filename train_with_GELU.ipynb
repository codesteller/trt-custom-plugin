{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy import pi\n",
    "from keras.backend import sigmoid\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Layer, InputLayer, Dense, Flatten, Activation, Conv2D, MaxPooling2D, LeakyReLU\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.backend import get_session\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import os\n",
    "from glob import glob\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "The following is the model class which has the methods to do training and prediction. It composes a simple convolution pooling and relu layers stacked to produce a image classification network. The dense/fully connected layer at the end has a custom layer GELU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,\n",
    "                 input_shape=(224, 224, 3),\n",
    "                 num_classes=2,\n",
    "                 checkpoint_path=\"./checkpoint\",\n",
    "                 batch_size=32,\n",
    "                 epochs=10,\n",
    "                 learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        input_shape - In HWC format\n",
    "        \"\"\"\n",
    "        self.model = Sequential()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        get_custom_objects().update({'gelu_activation': Activation(self.gelu_activation)})\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model.add(InputLayer(input_shape=self.input_shape))\n",
    "        self.model.add(Conv2D(32, kernel_size=(3, 3), activation='linear',\n",
    "                              input_shape=self.input_shape, padding='same'))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "        self.model.add(Conv2D(64, (3, 3), activation='linear', padding='same'))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "        self.model.add(\n",
    "            Conv2D(128, (3, 3), activation='linear', padding='same'))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(128, activation=\"linear\"))\n",
    "        self.model.add(Activation('gelu_activation', name='GeluActivation'))\n",
    "        self.model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "#         opt = Adam(learning_rate=self.learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        opt = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "\n",
    "        self.model.compile(optimizer=opt,\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "        print(\"Model built and compiled successfully\")\n",
    "\n",
    "    @staticmethod\n",
    "    def gelu_activation(_input, alpha=1):\n",
    "        return 0.5 * _input * (alpha + tf.tanh(tf.sqrt(2 / pi) * (_input + 0.044715 * _input * _input * _input)))\n",
    "\n",
    "    def train_model(self, train_data_gen, valid_data_gen):\n",
    "        checkpoint_dir = os.path.dirname(self.checkpoint_path)\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        tensorboard_dir = os.path.join(checkpoint_dir, 'tensorboard')\n",
    "        if not os.path.exists(tensorboard_dir):\n",
    "            os.makedirs(tensorboard_dir)\n",
    "\n",
    "        # Create a callback that saves the model's weights\n",
    "        cp_callback = ModelCheckpoint(filepath=self.checkpoint_path, save_weights_only=True, verbose=1, period=1)\n",
    "        # cp_callback = ModelCheckpoint(filepath=self.checkpoint_path, monitor='val_acc', verbose=1,\n",
    "        #                               save_best_only=True, mode='max')\n",
    "        tb_callback = TensorBoard(log_dir=tensorboard_dir, histogram_freq=0, write_graph=True, write_images=False)\n",
    "        self.model.fit_generator(\n",
    "            train_data_gen,\n",
    "            steps_per_epoch=train_data_gen.samples // self.batch_size,\n",
    "            epochs=self.epochs,\n",
    "            validation_data=valid_data_gen,\n",
    "            validation_steps=valid_data_gen.samples // self.batch_size,\n",
    "            callbacks=[cp_callback, tb_callback])\n",
    "\n",
    "    def convert_checkpoint(self, final_checkpoint):\n",
    "        checkpoint_dir = os.path.dirname(final_checkpoint)\n",
    "        basename = os.path.basename(final_checkpoint).split(\".\")[0]\n",
    "        save_path = os.path.join(checkpoint_dir, \"tf_ckpt\", \"final_model.ckpt\")\n",
    "        # Add ops to save and restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "        self.model.load_weights(final_checkpoint)\n",
    "        sess = get_session()\n",
    "        saver.save(sess, save_path)\n",
    "\n",
    "    def save_frozen(self, frozen_filename):\n",
    "        # First freeze the graph and remove training nodes.\n",
    "        output_names = self.model.output.op.name\n",
    "        sess = get_session()\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), [output_names])\n",
    "        frozen_graph = tf.graph_util.remove_training_nodes(frozen_graph)\n",
    "        # Save the model\n",
    "        with open(frozen_filename, \"wb\") as ofile:\n",
    "            ofile.write(frozen_graph.SerializeToString())\n",
    "\n",
    "    def prediction(self, test_data_path):\n",
    "        from PIL import Image\n",
    "        import numpy as np\n",
    "        test_images = glob(os.path.join(test_data_path, \"*.jpg\"))\n",
    "        for impath in test_images:\n",
    "            img = Image.open(impath)\n",
    "            img = img.resize(self.input_shape[:2])\n",
    "            img = np.expand_dims(np.array(img), axis=0) / 255.0\n",
    "            output = self.model.predict(img, batch_size=1)\n",
    "            print(output)\n",
    "            pred = np.argmax(output)\n",
    "            basename = os.path.basename(impath)\n",
    "            if pred:\n",
    "                print(\"{} : Prediction -  Dog\".format(basename))\n",
    "            else:\n",
    "                print(\"{} : Prediction -  Cat\".format(basename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset\n",
    "Following is the dataset class to prepare, preprocess and batch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBase:\n",
    "    def __init__(self, dbpath, input_shape=(224, 224, 3), batch_size=64):\n",
    "        \"\"\"\n",
    "\n",
    "        :param dbpath:  Path to the Dataset. DataBase works on a definite folder structure.\n",
    "                        Override the functions \"fetch_data_paths\" and \"data_generators\" for custom dataset.\n",
    "        :param input_shape: In format HWC\n",
    "        \"\"\"\n",
    "        self.dbpath = dbpath\n",
    "        self.input_shape = input_shape\n",
    "        self.train_dir = os.path.join(self.dbpath, 'train')\n",
    "        self.valid_dir = os.path.join(self.dbpath, 'valid')\n",
    "        self.im_height = input_shape[0]\n",
    "        self.im_width = input_shape[1]\n",
    "        self.channels = input_shape[2]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.fetch_data_paths()\n",
    "\n",
    "    def fetch_data_paths(self):\n",
    "        train_cats_dir = os.path.join(self.train_dir, 'cats')  # directory with our training cat pictures\n",
    "        train_dogs_dir = os.path.join(self.train_dir, 'dogs')  # directory with our training dog pictures\n",
    "        validation_cats_dir = os.path.join(self.valid_dir, 'cats')  # directory with our validation cat pictures\n",
    "        validation_dogs_dir = os.path.join(self.valid_dir, 'dogs')  # directory with our validation dog pictures\n",
    "\n",
    "        num_cats_tr = len(os.listdir(train_cats_dir))\n",
    "        num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
    "\n",
    "        num_cats_val = len(os.listdir(validation_cats_dir))\n",
    "        num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
    "\n",
    "        total_train = num_cats_tr + num_dogs_tr\n",
    "        total_val = num_cats_val + num_dogs_val\n",
    "\n",
    "        print('total training cat images:', num_cats_tr)\n",
    "        print('total training dog images:', num_dogs_tr)\n",
    "\n",
    "        print('total validation cat images:', num_cats_val)\n",
    "        print('total validation dog images:', num_dogs_val)\n",
    "        print(\"--\")\n",
    "        print(\"Total training images:\", total_train)\n",
    "        print(\"Total validation images:\", total_val)\n",
    "\n",
    "    def data_generators(self):\n",
    "        # Prepare Training Data\n",
    "        train_image_generator = ImageDataGenerator(rotation_range=40,\n",
    "                                                   width_shift_range=0.2,\n",
    "                                                   height_shift_range=0.2,\n",
    "                                                   shear_range=0.2,\n",
    "                                                   zoom_range=0.2,\n",
    "                                                   channel_shift_range=10,\n",
    "                                                   horizontal_flip=True,\n",
    "                                                   fill_mode='nearest',\n",
    "                                                   rescale=1. / 255)\n",
    "        train_batches = train_image_generator.flow_from_directory(self.train_dir,\n",
    "                                                                  target_size=(self.im_height, self.im_width),\n",
    "                                                                  interpolation='bicubic',\n",
    "                                                                  class_mode='categorical',\n",
    "                                                                  shuffle=True,\n",
    "                                                                  batch_size=self.batch_size)\n",
    "\n",
    "        # Prepare Validation Data\n",
    "        valid_image_generator = ImageDataGenerator(rescale=1. / 255)\n",
    "        valid_batches = valid_image_generator.flow_from_directory(self.train_dir,\n",
    "                                                                  target_size=(self.im_height, self.im_width),\n",
    "                                                                  interpolation='bicubic',\n",
    "                                                                  class_mode='categorical',\n",
    "                                                                  shuffle=True,\n",
    "                                                                  batch_size=self.batch_size)\n",
    "\n",
    "        return train_batches, valid_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "learning_rate = 0.001\n",
    "input_shape = (150, 150, 3)\n",
    "\n",
    "# Setup Directories\n",
    "checkpoint_path = \"./saved_model/checkpoints/saved_model-{epoch:04d}.h5\"\n",
    "dataset_dir = \"/datasets/dogs_cats/train_data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images: 10000\n",
      "total training dog images: 10000\n",
      "total validation cat images: 2500\n",
      "total validation dog images: 2500\n",
      "--\n",
      "Total training images: 20000\n",
      "Total validation images: 5000\n",
      "Found 20000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Prepare Dataset\n",
    "db = DataBase(dataset_dir, input_shape=input_shape, batch_size=batch_size)\n",
    "train_generator, valid_generator = db.data_generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built and compiled successfully\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 150, 150, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 150, 150, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 75, 75, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 38, 38, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 38, 38, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 46208)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               5914752   \n",
      "_________________________________________________________________\n",
      "GeluActivation (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 6,008,258\n",
      "Trainable params: 6,008,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "cnn_model = Model(input_shape=input_shape,\n",
    "                  checkpoint_path=checkpoint_path,\n",
    "                  epochs=epochs,\n",
    "                  learning_rate=learning_rate,\n",
    "                  batch_size=batch_size)\n",
    "cnn_model.build_model()\n",
    "cnn_model.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 322s 2s/step - loss: 0.7019 - acc: 0.5642 - val_loss: 0.6731 - val_acc: 0.5534\n",
      "\n",
      "Epoch 00001: saving model to ./saved_model/checkpoints/saved_model-0001.h5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:995: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "[[0.40308756 0.59691244]]\n",
      "cat.8.jpg : Prediction -  Dog\n",
      "[[0.3197724  0.68022764]]\n",
      "cat.7.jpg : Prediction -  Dog\n",
      "[[0.5552775  0.44472244]]\n",
      "cat.10.jpg : Prediction -  Cat\n",
      "[[0.04763709 0.95236295]]\n",
      "dog.4.jpg : Prediction -  Dog\n",
      "[[0.3868808  0.61311924]]\n",
      "dog.3.jpg : Prediction -  Dog\n",
      "[[0.39767185 0.6023282 ]]\n",
      "cat.5.jpg : Prediction -  Dog\n",
      "[[0.35000253 0.64999753]]\n",
      "cat.6.jpg : Prediction -  Dog\n",
      "[[0.3325425  0.66745746]]\n",
      "dog.2.jpg : Prediction -  Dog\n",
      "[[0.35357457 0.64642537]]\n",
      "dog.1.jpg : Prediction -  Dog\n",
      "[[0.363185  0.6368151]]\n",
      "cat.9.jpg : Prediction -  Dog\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "cnn_model.train_model(train_generator, valid_generator)\n",
    "cnn_model.prediction(\"./test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Keras Model to Tensorflow Checkpoint\n",
    "str_epoch = str(epochs).zfill(4)\n",
    "final_checkpoint = \"saved_model/checkpoints/saved_model-{}.h5\".format(str_epoch)\n",
    "cnn_model.convert_checkpoint(final_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
